# yaml-language-server: $schema=https://taskfile.dev/schema.json
version: '3'

tasks:
  node-shell:
    desc: Open a shell to a node [NODE=required]
    interactive: true
    cmd: kubectl node-shell -n kube-system -x {{.NODE}}
    requires:
      vars: [NODE]
    preconditions:
      - kubectl get nodes {{.NODE}}
      - kubectl node-shell --version
      - which kubectl

  sync-secrets:
    desc: Sync all ExternalSecrets
    cmds:
      - for: { var: SECRETS, split: "\n" }
        cmd: kubectl --namespace {{splitList "," .ITEM | first}} annotate externalsecret {{splitList "," .ITEM | last}} force-sync="{{now | unixEpoch}}" --overwrite
    vars:
      SECRETS:
        sh: kubectl get externalsecret --all-namespaces --no-headers --output=jsonpath='{range .items[*]}{.metadata.namespace},{.metadata.name}{"\n"}{end}'
    preconditions:
      - which kubectl

  cleanse-pods:
    desc: Cleanse pods with a Failed/Pending/Succeeded phase
    cmds:
      - for:
          matrix:
            PHASE: [Failed, Pending, Succeeded]
        cmd: kubectl delete pods --all-namespaces --field-selector status.phase={{.ITEM.PHASE}} --ignore-not-found=true
    preconditions:
      - which kubectl

  # https://docs.github.com/en/enterprise-cloud@latest/actions/hosting-your-own-runners/managing-self-hosted-runners-with-actions-runner-controller/deploying-runner-scale-sets-with-actions-runner-controller#upgrading-arc
  upgrade-arc:
    desc: Upgrade the ARC
    cmds:
      - helm -n actions-runner-system uninstall home-ops-runner
      - helm -n actions-runner-system uninstall actions-runner-controller
      - sleep 5
      - flux -n actions-runner-system reconcile hr actions-runner-controller
      - flux -n actions-runner-system reconcile hr home-ops-runner
    preconditions:
      - which flux helm

  vnc-connect:
    desc: "Connect to a running VM via virtctl VNC"
    silent: true
    cmds:
      - |
        #!/usr/bin/env bash
        set +e  # disable exit on error so we can handle errors manually

        selected_vm=$(kubectl get vms --all-namespaces -o json | \
          jq -r '.items[] | select(.status.printableStatus == "Running") | "\(.metadata.namespace)\t\(.metadata.name)"' | \
          fzf --with-nth=2 --preview='echo Namespace: {1}' --prompt="ðŸ–¥ Select a running VM: ")

        fzf_exit_code=$?

        if [[ $fzf_exit_code -ne 0 || -z "$selected_vm" ]]; then
          echo "âŒ No VM selected or selection cancelled."
          exit 0   # EXIT 0 so the task runner knows this is NOT an error
        fi

        namespace=$(echo "$selected_vm" | awk '{print $1}')
        vm_name=$(echo "$selected_vm" | awk '{print $2}')

        if [[ -z "$namespace" || -z "$vm_name" ]]; then
          echo "âŒ Invalid VM selection."
          exit 0
        fi

        echo "ðŸ”Œ Connecting to VMI $vm_name in namespace $namespace..."
        virtctl vnc "$vm_name" -n "$namespace"
        exit_code=$?

        # exit with virtctl's exit code or 0 if virtctl succeeded
        if [[ $exit_code -ne 0 ]]; then
          echo "âŒ virtctl command failed with exit code $exit_code"
        fi

        exit $exit_code
    preconditions:
      - which kubectl virtctl fzf jq

  start-pgadmin:
    desc: Start pgAdmin Docker container
    silent: true
    cmds:
      - |
        echo "ðŸ”Œ Starting port-forward..."
        kubectl port-forward svc/postgres-v17-rw 5432:5432 -n database >/dev/null 2>&1 &

        echo "ðŸš€ Starting pgAdmin..."
        docker run --rm --name pgadmin \
          --add-host=host.docker.internal:host-gateway \
          -e PGADMIN_DEFAULT_EMAIL=georgi.panov@darkfellanetwork.com \
          -e PGADMIN_DEFAULT_PASSWORD=admin \
          -e PGADMIN_REPLACE_SERVERS_ON_STARTUP=True \
          -e PGPASS_FILE=/.pgpass \
          -v /home/darkfella/home-ops/.pgadmin/servers.json:/pgadmin4/servers.json \
          -v /home/darkfella/home-ops/.pgadmin/.pgpass:/.pgpass \
          -p 8080:80 \
          dpage/pgadmin4:9.6.0

    preconditions:
      - which docker kubectl
