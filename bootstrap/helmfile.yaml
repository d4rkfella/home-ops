---
# yaml-language-server: $schema=https://json.schemastore.org/helmfile

helmDefaults:
  cleanupOnFail: true
  wait: true
  waitForJobs: true
  # waitRetries: 3 # Not supported by Helm yet

releases:

  - name: cilium
    namespace: kube-system
    atomic: true
    chart: oci://ghcr.io/d4rkfella/charts-mirror/cilium
    version: 1.17.2
    values: ['{{ requiredEnv "KUBERNETES_DIR" }}/apps/kube-system/cilium/manifests/helm/values.yaml']
    hooks:
      - events: ["prepare"]
        command: bash
        args:
          - -c
          - |
            urls=$(yq -N '.crds[] | select(test("^https"))' {{ requiredEnv "BOOTSTRAP_DIR" }}/crds.yaml)

            for url in $urls; do
              echo "Applying CRD from $url"
              kubectl apply --server-side -f $url
            done
            echo "All CRDs applied successfully!"
        showlogs: true
      - events: ['postsync']
        command: bash
        args:
          - -c
          - until kubectl get crd ciliuml2announcementpolicies.cilium.io ciliumloadbalancerippools.cilium.io kubevirts.kubevirt.io cdis.cdi.kubevirt.io &>/dev/null; do sleep 10; done
        showlogs: true
      - events: ['postsync']
        command: kubectl
        args:
          - apply
          - --namespace=kube-system
          - --server-side
          - --field-manager=kustomize-controller
          - --filename={{ requiredEnv "KUBERNETES_DIR" }}/apps/kube-system/cilium/manifests/networks.yaml
        showlogs: true

  - name: coredns
    namespace: kube-system
    atomic: true
    chart: oci://ghcr.io/coredns/charts/coredns
    version: 1.42.3
    values: ['{{ requiredEnv "KUBERNETES_DIR" }}/apps/kube-system/coredns/manifests/helm/values.yaml']
    needs: ['kube-system/cilium']

  - name: kubelet-csr-approver
    namespace: kube-system
    atomic: true
    chart: oci://ghcr.io/d4rkfella/charts-mirror/kubelet-csr-approver
    version: 1.2.7
    values: ['{{ requiredEnv "KUBERNETES_DIR" }}/apps/kube-system/kubelet-csr-approver/manifests/helm/values.yaml']
    needs: ['kube-system/coredns']

  - name: cert-manager
    namespace: cert-manager
    atomic: true
    chart: oci://ghcr.io/d4rkfella/charts-mirror/cert-manager
    version: v1.17.2
    values: ['{{ requiredEnv "KUBERNETES_DIR" }}/apps/cert-manager/cert-manager/manifests/helm/values.yaml']
    needs: ['kube-system/kubelet-csr-approver']
    hooks:
      - events: ["prepare"]
        command: bash
        args:
          - -c
          - |
            if ! kubectl get namespace "{{ .Release.Namespace }}" &>/dev/null; then
              kubectl create namespace "{{ .Release.Namespace }}"
            fi
        showlogs: true
      - events: ['postsync']
        command: bash
        args:
          - -c
          - until kubectl get crd clusterissuers.cert-manager.io certificates.cert-manager.io &>/dev/null; do sleep 10; done
        showlogs: true
      - events: ['postsync']
        command: bash
        args:
          - -c
          - sops -d {{ requiredEnv "KUBERNETES_DIR" }}/apps/cert-manager/cert-manager/manifests/issuer.secret.sops.yaml | kubectl apply --server-side --filename - --namespace cert-manager
      - events: ['postsync']
        command: kubectl
        args:
          - apply
          - --server-side
          - --field-manager=kustomize-controller
          - --filename={{ requiredEnv "KUBERNETES_DIR" }}/apps/cert-manager/cert-manager/manifests/clusterissuer.yaml
        showlogs: true
      - events: ['postsync']
        command: kubectl
        args:
          - apply
          - --namespace=vault
          - --server-side
          - --field-manager=kustomize-controller
          - --filename={{ requiredEnv "KUBERNETES_DIR" }}/apps/vault/vault/manifests/certificate.yaml

  - name: external-secrets
    namespace: external-secrets
    atomic: true
    chart: oci://ghcr.io/external-secrets/charts/external-secrets
    version: 0.17.0
    values: ['{{ requiredEnv "KUBERNETES_DIR" }}/apps/external-secrets/external-secrets/manifests/helm/values.yaml']
    needs: ['vault/vault','cert-manager/cert-manager']
    hooks:
      - events: ["prepare"]
        command: bash
        args:
          - -c
          - |
            if ! kubectl get namespace "{{ .Release.Namespace }}" &>/dev/null; then
              kubectl create namespace "{{ .Release.Namespace }}"
            fi
        showlogs: true
      - events: ['postsync']
        command: bash
        args:
          - -c
          - until kubectl get crd clustersecretstores.external-secrets.io &>/dev/null; do sleep 10; done
        showlogs: true
      - events: ['postsync']
        command: kubectl
        args:
          - apply
          - --namespace=external-secrets
          - --server-side
          - --field-manager=kustomize-controller
          - --filename={{ requiredEnv "KUBERNETES_DIR" }}/apps/external-secrets/external-secrets/manifests/clustersecretstore.yaml
        showlogs: true

  - name: zfs-localpv
    namespace: openebs-system
    atomic: true
    chart: oci://ghcr.io/d4rkfella/charts-mirror/zfs-localpv
    version: 2.8.0-develop
    values: ['{{ requiredEnv "KUBERNETES_DIR" }}/apps/openebs-system/zfs-localpv/manifests/helm/values.yaml']
    needs: ['kube-system/cilium','kube-system/coredns']
    hooks:
      - events: ["prepare"]
        command: bash
        args:
          - -c
          - |
            if ! kubectl get namespace "{{ .Release.Namespace }}" &>/dev/null; then
              kubectl create namespace "{{ .Release.Namespace }}"
            fi
        showlogs: true
      - events: ['presync']
        command: kubectl
        args:
          - apply
          - --namespace=kube-system
          - --server-side
          - --field-manager=kustomize-controller
          - --filename={{ requiredEnv "BOOTSTRAP_DIR" }}/create-zpool.yaml
        showlogs: true
      - events: ['postsync']
        command: bash
        args:
          - -c
          - until kubectl get crd volumesnapshotclasses.snapshot.storage.k8s.io &>/dev/null; do sleep 10; done
        showlogs: true
      - events: ['postsync']
        command: kubectl
        args:
          - apply
          - --server-side
          - --field-manager=kustomize-controller
          - --filename={{ requiredEnv "KUBERNETES_DIR" }}/apps/openebs-system/zfs-localpv/manifests/storageclass.yaml,{{ requiredEnv "KUBERNETES_DIR" }}/apps/openebs-system/zfs-localpv/manifests/volumesnapshotclass.yaml
        showlogs: true

  - name: vault
    namespace: vault
    atomic: true
    chart: oci://ghcr.io/d4rkfella/charts-mirror/vault
    version: 0.30.0
    values: ['{{ requiredEnv "KUBERNETES_DIR" }}/apps/vault/vault/manifests/helm/values.yaml']
    needs: ['cert-manager/cert-manager','openebs-system/zfs-localpv']
    hooks:
    - events: ["prepare"]
      command: bash
      args:
        - -c
        - |
          if ! kubectl get namespace "{{ .Release.Namespace }}" &>/dev/null; then
            kubectl create namespace "{{ .Release.Namespace }}"
          fi
      showlogs: true
    - events: ['prepare']
      command: bash
      args:
        - -c
        - sops -d {{ requiredEnv "KUBERNETES_DIR" }}/apps/vault/vault/manifests/vault.secret.sops.yaml | kubectl apply --server-side --filename - --namespace "{{ .Release.Namespace }}"
      showlogs: true
    - events: ["postsync"]
      command: bash
      args:
        - -c
        - |
          echo "Waiting for Vault pods to be Running..."
          while true; do
            VAULT_PODS=$(kubectl get pods -n vault -l app.kubernetes.io/name=vault -o jsonpath='{.items[*].status.phase}')
            if echo "$VAULT_PODS" | grep -q "Running" && ! echo "$VAULT_PODS" | grep -q "Pending"; then
              echo "All Vault pods are Running."
              break
            else
              echo "Vault pods are not yet Running. Waiting..."
              sleep 10
            fi
          done

          export VAULT_ADDR=https://vault.darkfellanetwork.com:8200
          vault operator init > /tmp/vault-keys.json
      showlogs: true
    - events: ["postsync"]
      command: bash
      args:
        - -c
        - |
          echo "Waiting for Vault to become unsealed..."
          export VAULT_ADDR=https://vault.darkfellanetwork.com:8200
          while true; do
            VAULT_STATUS=$(vault status 2>/dev/null)
            if [ $? -eq 0 ]; then
              SEALED_STATUS=$(echo "$VAULT_STATUS" | grep "Sealed" | awk '{print $2}')
              if [ "$SEALED_STATUS" == "false" ]; then
                echo "Vault is unsealed. Proceeding with snapshot restoration..."
                break
              else
                echo "Vault is still sealed. Retrying in 5 seconds..."
                sleep 5
              fi
            else
              echo "Error: Unable to retrieve Vault status. Retrying in 5 seconds..."
              sleep 5
            fi
          done

          export VAULT_TOKEN=$(grep 'Initial Root Token:' /tmp/vault-keys.json | awk '{print $4}')

          vault-backup restore --force --config=/project/.vault/vault-backup.yaml --vault-token=$VAULT_TOKEN
      showlogs: true

  - name: multus
    namespace: kube-system
    atomic: true
    chart: oci://ghcr.io/bjw-s-labs/helm/app-template
    version: 4.1.1
    values: ['{{ requiredEnv "KUBERNETES_DIR" }}/apps/kube-system/multus/manifests/helm/values.yaml']
    needs: ['kube-system/cilium','kube-system/coredns']
    hooks:
    - events: ["postsync"]
      command: kubectl
      args:
      - apply
      - --server-side
      - --field-manager=kustomize-controller
      - --filename={{ requiredEnv "KUBERNETES_DIR" }}/apps/kube-system/multus/manifests/rbac.yaml
      showlogs: true
    - events: ["postsync"]
      command: kubectl
      args:
        - apply
        - --namespace=kubevirt
        - --server-side
        - --field-manager=kustomize-controller
        - --filename={{ requiredEnv "KUBERNETES_DIR" }}/apps/kubevirt/kubevirt-instance/manifests/kubevirt.yaml
      showlogs: true
    - events: ["postsync"]
      command: kubectl
      args:
        - apply
        - --namespace=cdi
        - --server-side
        - --field-manager=kustomize-controller
        - --filename={{ requiredEnv "KUBERNETES_DIR" }}/apps/cdi/cdi-instance/manifests/cdi.yaml
      showlogs: true
    - events: ["postsync"]
      command: bash
      args:
        - -c
        - until kubectl get crd storageprofiles.cdi.kubevirt.io &>/dev/null; do sleep 10; done
      showlogs: true
    - events: ["postsync"]
      command: kubectl
      args:
        - apply
        - --server-side
        - --field-manager=kustomize-controller
        - --filename={{ requiredEnv "KUBERNETES_DIR" }}/apps/cdi/cdi-instance/manifests/storageprofile.yaml
      showlogs: true
    - events: ["postsync"]
      command: bash
      args:
        - -c
        - |
          if ! kubectl get namespace virtualization &>/dev/null; then
            kubectl create namespace virtualization
          fi
      showlogs: true
    - events: ['postsync']
      command: bash
      args:
        - -c
        - sops -d {{ requiredEnv "KUBERNETES_DIR" }}/apps/virtualization/windows-server/manifests/sysprep.secret.sops.yaml | kubectl apply --server-side --filename - --namespace virtualization
      showlogs: true
    - events: ["postsync"]
      command: bash
      args:
        - -c
        - |
          echo "Checking if all required deployments and daemonsets are running..."

          KUBEVIRT_DEPLOYMENTS=("virt-operator" "virt-controller" "virt-api")
          KUBEVIRT_DAEMONSETS=("virt-handler")
          CDI_DEPLOYMENTS=("cdi-uploadproxy" "cdi-operator" "cdi-deployment" "cdi-apiserver")

          MAX_ATTEMPTS=12
          SLEEP_INTERVAL="30s"

          check_and_retry_deployment_status() {
            local NAMESPACE=$1
            local DEPLOYMENT_NAME=$2
            local ATTEMPTS=0

            echo "Checking deployment $DEPLOYMENT_NAME in namespace $NAMESPACE..."
            while true; do
              kubectl -n "$NAMESPACE" rollout status "deployment/$DEPLOYMENT_NAME" --timeout=1m
              if [ $? -eq 0 ]; then
                echo "Deployment $DEPLOYMENT_NAME in namespace $NAMESPACE is running."
                return 0 # Success
              fi

              ATTEMPTS=$((ATTEMPTS + 1))
              if [ "$ATTEMPTS" -ge "$MAX_ATTEMPTS" ]; then
                echo "Deployment $DEPLOYMENT_NAME in namespace $NAMESPACE failed to roll out after $MAX_ATTEMPTS attempts. Exiting."
                exit 1 # Failed after max attempts
              fi

              echo "Deployment $DEPLOYMENT_NAME in namespace $NAMESPACE not ready yet. Retrying in $SLEEP_INTERVAL (attempt $ATTEMPTS/$MAX_ATTEMPTS)..."
              sleep "$SLEEP_INTERVAL"
            done
          }

          # --- Function to check and retry daemonset status ---
          check_and_retry_daemonset_status() {
            local NAMESPACE=$1
            local DAEMONSET_NAME=$2
            local ATTEMPTS=0

            echo "Checking daemonset $DAEMONSET_NAME in namespace $NAMESPACE..."
            while true; do
              kubectl -n "$NAMESPACE" rollout status "daemonset/$DAEMONSET_NAME" --timeout=1m
              if [ $? -eq 0 ]; then
                echo "Daemonset $DAEMONSET_NAME in namespace $NAMESPACE is running."
                return 0 # Success
              fi

              ATTEMPTS=$((ATTEMPTS + 1))
              if [ "$ATTEMPTS" -ge "$MAX_ATTEMPTS" ]; then
                echo "Daemonset $DAEMONSET_NAME in namespace $NAMESPACE failed to roll out after $MAX_ATTEMPTS attempts. Exiting."
                exit 1 # Failed after max attempts
              fi

              echo "Daemonset $DAEMONSET_NAME in namespace $NAMESPACE not ready yet. Retrying in $SLEEP_INTERVAL (attempt $ATTEMPTS/$MAX_ATTEMPTS)..."
              sleep "$SLEEP_INTERVAL"
            done
          }

          # --- Check Kubevirt Deployments ---
          for DEPLOYMENT in "${KUBEVIRT_DEPLOYMENTS[@]}"; do
            check_and_retry_deployment_status "kubevirt" "$DEPLOYMENT"
          done

          # --- Check Kubevirt Daemonsets ---
          for DAEMONSET in "${KUBEVIRT_DAEMONSETS[@]}"; do
            check_and_retry_daemonset_status "kubevirt" "$DAEMONSET"
          done

          # --- Check CDI Deployments ---
          for DEPLOYMENT in "${CDI_DEPLOYMENTS[@]}"; do
            check_and_retry_deployment_status "cdi" "$DEPLOYMENT"
          done

          echo "All required deployments and daemonsets are running."
      showlogs: true
    - events: ["postsync"]
      command: bash
      args:
        - -c
        - until kubectl get crd virtualmachines.kubevirt.io datavolumes.cdi.kubevirt.io &>/dev/null; do sleep 10; done
      showlogs: true
    - events: ["postsync"]
      command: kubectl
      args:
        - apply
        - --namespace=virtualization
        - --server-side
        - --field-manager=kustomize-controller
        - --filename={{ requiredEnv "KUBERNETES_DIR" }}/apps/virtualization/windows-server/manifests/multus-network.yaml
        - --filename={{ requiredEnv "KUBERNETES_DIR" }}/apps/virtualization/windows-server/manifests/virtualmachine.yaml
        - --filename={{ requiredEnv "KUBERNETES_DIR" }}/apps/virtualization/truenas-scale/manifests/multus-network.yaml
        - --filename={{ requiredEnv "KUBERNETES_DIR" }}/apps/virtualization/truenas-scale/manifests/virtualmachine.yaml
      showlogs: true

  - name: envoy-gateway-controller
    namespace: envoy-gateway-system
    atomic: true
    chart: oci://docker.io/envoyproxy/gateway-helm
    version: 1.4.1
    needs: ['cert-manager/cert-manager','kube-system/cilium']
    hooks:
    - events: ["prepare"]
      command: bash
      args:
        - -c
        - |
          if ! kubectl get namespace "{{ .Release.Namespace }}" &>/dev/null; then
            kubectl create namespace "{{ .Release.Namespace }}"
          fi
      showlogs: true
